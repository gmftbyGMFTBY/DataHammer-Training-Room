## Advanced Word Vector

1. review word vector 

   $p(u|v)=\frac{\exp(u^Tv)}{\sum_s \exp(u_s^Tv)}$

   利用语料库中的词共现的特性，最大化 $p$ 的似然，精准的预测一个词的上下文单词出现的概率

   其中 $v$ 是中心词，$u$ 是上下文单词。分母的计算量非常大，可以预计算。

2. 普遍认为Hierarchical Softmax对低频词效果较好；Negative Sampling对高频词效果较好，向量维度较低时效果更好。

3. 负采样

   传统的计算上面的式子的时候我们需要对所有的单词都进行 $p$ 的更新，但是实际上词共现是非常稀疏的，因此使用负采样的思路可以极大的提高计算效率。损失函数如下
   $$
   J(\theta)=\frac{1}{T}\sum_{t=1}^TJ_t(\theta)
   $$
   上式中，$T$ 的含义是窗口的数目，可以理解成我们要考虑的每一个中心词
   $$
   J_t(\theta)=\log \sigma(u_o^Tv_c)+\sum_{i=1}^k \mathbb{E}_{j\backsim P(w)}[\log \sigma(-u_j^Tv_c)]
   $$
   上述的第二项其实本质上可以使用负采样的方法随机的抽选 $k$ 个负样本计算
   $$
   J_t(\theta)=\log \sigma(u_o^Tv_c)+\sum_{j\backsim P(w)}[\log \sigma(-u_j^Tv_c)]=\log \sigma(u_o^Tv_c)+\sum_{j\backsim P(w)}[\log (1-\sigma(u_j^Tv_c))]
   $$
   其中 $u_o$ 是共现的单词,$u_j$ 是随机的认为不共现的单词，最大化 $J_t(\theta)$ 意味着让共现单词的概率尽量大，不共现的单词的概率尽量小。其中每一个负采样是利用分布 $P(w)=\frac{U(w)^{\frac{3}{4}}}{Z}$ 计算得到的，指数目的是尽量采样稀有的词保证不相关。

4. 词向量的目的在于把相似的单词在超空间聚集一起

5. 之前的良好的词向量的方法是 LSA (latent semantic analise) 用共现次数矩阵作为一个主要的信息，但是和传统的 one-hot 编码具有相同的缺点就是维度太高，所以可以使用 SVD 进行矩阵分解，降低维度。

   * 常用的词非常高频，比如 he, the 等词这时候对次数做一个上限约束，词的长尾效应(稀少的词往往具有更多的语义信息)
   * 随机窗口等等方法
   * SVD 复杂度很高，计算代价给很大

6. 基于计数方法和直接预测方法(word2vec)方法之间的区别

   * w2v 难以利用语料库统计信息，随着语料库的大小而改变计算复杂度，对下游任务有更好的效果捕获的信息更多(各种各样的复杂模式)
   * 基于计数的方法训练快速，高效利用统计信息

7. GloVe (Global vector)

   * 训练快速，克服了 SVD 的缺陷
   * 巨大语料的扩展性
   * 性能良好
   * 结合两者的优点

   $$
   J(\theta)=\frac{1}{2}\sum_{i,j=1}^Wf(P_{ij})(u_i^Tv_j-\log P_{ij})^2
   $$

   $P$ 是巨大的共现矩阵($P_{ij}$ 代表中心词 $i$ 和上下文单词 $j$ 在特定窗口大小内共现的次数)，计算语料库中每一对词的共现次数，希望最小化内积距离和共现频率的差异，$f$ 函数可以减少一些异常平凡的共现词的数目(非递减函数，不希望这个权重过大数目增大到一定程度就不增加，当两个词完全不共县，没有必要优化)，将其固定在一个上限值上。

   $f$ 函数常用的形式如下
   $$
   f(x)=\left\{\begin{array}{rcl}
   (\frac{x}{x_{max}})^\alpha,if\ x<x_{max}\\
   1,otherwise
   \end{array}\right.
   $$
   ![](..\photo\3.png)

   $x_{max}$ 论文中取值是 100

8. 评估词向量

   * 内部评估: 评估特定的指标和任务，但是对实际的任务没有指导意义，快速

     **有时候词向量维度大了效果反而会下降**

   * 外部评估: 针对实际任务进行效果的评估，缓慢

---

分层 Softmax

1. 哈夫曼树

   哈夫曼树是按照权重不断的合并子树构建的一棵最小权重树，我们一般会对哈夫曼树的叶子节点进行编码，权重高的节点靠近根节点，高权重的节点编码短，从而保证树的路径最短

2. 分层 Softmax

   传统的使用 word2vec 中最耗时的部分在于从隐藏层到输出层的计算过程中，这一部分的计算复杂度最大，使用分层 Softmax 的方法解决。

   __CBOW__

   1. 输入层到隐藏层中的计算并不适用 NN 模型，比如 Skip-gram 中，对所有的上下文向量取平均代表，得到一个向量。

   2. 为了避免在最后的隐含层到输出层计算所有单词的 softmax 概率，这里采用哈夫曼树代替隐藏层到输出层的映射

      ![](..\photo\4.png)

      在这个哈夫曼树中，我们规定向左走是负类1向右走是0正类，使用 $\sigma$ 来计算对应的概率分布

      $P(+)=\sigma(x^T	\theta)=\frac{1}{1+e^{-x^T\theta}}\\P(-)=1-P(+)$

      每一个非叶子节点存在有一个对应的 softmax 模型参数

      计算最后的单词的概率是 $P(w)=\Pi_{i=1} P(n(w,i))$

   3. 梯度更新

      CBOW 更新的时候是所有的窗口内的词向量都更新

   __Skip-gram__

   基本和 CBOW 一致，但是 CBOW 的输入输出是 $(context(w),w)$ skip-gram 的输入输出是窗口内的每一个 $(w,context(w))$