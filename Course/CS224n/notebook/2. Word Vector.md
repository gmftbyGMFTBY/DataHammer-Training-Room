## Word Vector 词向量

1. 在词向量之前，对于单词的语义信息都是分类存储的，比如最有名就是 NLTK (Nature Language Tool Kit) 和 WordNet 等分类存储的单词语义信息和处理工具，但是这样的分类的存储工具的一个巨大的缺点就是不完整不灵活，并且存在有信息的确实(相似性信息)

   之前的 NLP 都是使用的 one-hot 编码表示，这种表示的方法只记录了单词在词典中的位置信息，损失了大量的外部语义。

2. 为了对语义的理解做出更大的提升，一个主要的理论叫做分布相似性：

   **语义是由上下文的单词的语义构成表示的(词的共现特性)**

   为了适合计算，我们采用低维向量进行词的描述，记为词向量 word vector

3. 重要的方法

   2013 年 Google 的研究员提出了 word2vector 词向量高效计算的两种算法和对应的两种优化方式，并公开了 C 语言版本的开源软件，word2vector 方法正式出现。

   这两种算法分别是 skip-gram (中心词预测上下文单词) 和 CBOW (上下文单词预测中心词) ,对应的两种优化方法分别是分层 Softmax 和负采样。

   这一节课主要从 skip-gram 出发讲解 w2v 算法的本质，skip-gram 算法非常简单

   1. 选取位置 $t$ 的单词作为中心词中心词 $w_t$ 

   2. 预测 $w_t$ 在一定范围内 (window size $m$) 下的上下文词汇，本质上就是计算词典中上下文单词在 $w_t$ 上下文中出现的概率。

   3. 调整中心词的词向量，优化概率分布
      $$
      J'(\theta)=\Pi_{t=1}^T\Pi_{-m\leq j\leq m,j \neq 0} p(w_{t+j}|w_t;\theta)\\
      J(\theta)=-\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m,j\neq 0} \log p(w_{t+j}|w_t)
      $$
      这个损失函数的意思就是，其中 $\theta$ 的含义是对应的 lookup table，也是模型中唯一的可训练参数。每一个参数都是就是每一个单词对应的词向量。

      计算的时候，首先获得一个中间词，然后对中心词的上下文进行预测。上述的概率 $p$ 就是简单的 softmax 归一化的概率分布。

      $p=\frac{e^{u^Tv}}{\sum_s e^{u_s^Tv}}$

   4. 优化方法

      优化方法就是简单的梯度下降
      $$
      \frac{\partial{\log \frac{e^{u_c^Tv}}{\sum_s e^{u_s^Tc}}}}{\partial{v}}
      $$
      上述就是对应的 $J(\theta)$ 的函数的 softmax 的展开形式
      $$
      \frac{\partial{J(\theta)}}{\partial{\theta}}=u_c - \sum_{x=1}^v p(x|c)u_x
      $$
      上述就是对应的梯度计算结果，$u_c$ 的含义即使当前的上下文的词向量，后面的求和式是每一个上下文单词的词向量期望，梯度下降的目的就是让 $u_c=\sum_{x=1}^vp(x|c)u_x$ 目的是让上下文单词的词向量和期望尽可看那个的匹配。这是他的宏观的解释方式，其实本质上的词向量的解释方式非常的简单，就是单纯的利用中心词的词向量精准的预测上下文单词，这样的词向量表示可以有效的刻画单词共现的性质。

   5. 模型架构

      ![](..\photo\2.PNG)

