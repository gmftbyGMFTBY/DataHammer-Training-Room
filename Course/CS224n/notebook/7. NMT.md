## 神经机器翻译

1. 目前的机器翻译都是基于统计的，概率公式如下

   $e=\arg \max_ep(e|f)=\arg \max_e p(f|e)p(e)$

   其中 $e$ 是目标语句， $f$ 是源语句，其中 $p(f|e)$ 是翻译模型，$p(e)$ 是语言模型，翻译模型和语言模型的乘积的含义在于我们对于一个翻译源语句 $f$ 可以根据统计得到大量的 $e$ 候选语句，最好的那一个 $p(e)$ 就是符合 $e$ 语言模型 ($p(e)$ 大)并且 $p(f|e)$ 也很大的一个目标语句(符合含义)。

2. 翻译模型

   对齐是翻译模型的核心，找到不同语言中单词或者词组的对应关系(有时候甚至没有对应关系或者存在一对多关系，多对一，多对多)。传统解决这个问题的方法如果使用组合数来做的话会面对组合爆炸的问题。

3. 反向输入语句给 s2s 的话也是可以得到一样的效果但是我们热内未必是这样看待的，老师在课堂上的即使是我们都是根据目标函数来优化的，这属于内部的参数调整的问题，我们学习到的是一个统一的函数近似器和输入的序列是正序还是逆序没有关系。在 GRU 的鲁论文中使用的是逆序的方法的原因在于避免长时依赖的出现，得到更好的开头。

4. 高级循环神经网络

   1. GRU

      门结构中存在有两个主要的门，一个是更新门 $z$ 一个是重置门 $r$ 
      $$
      z_t=\sigma(W^{(z)}x_t + U^{(z)}h_{t-1})\\
      r_t=\sigma(W^{(r)}x_t + U^{(r)}h_{t-1})\\
      \overline{h_t}=\tanh(Wx_t + r_t \cdot Uh_{t-1})\\
      h_t = z_t \cdot h_{t-1} + (1-z_t)\cdot \overline{h_t}
      $$
      其中重置门的作用在于在当前的隐状态生成的时候决定是否忽略之前的一些特征，更新门的作用在于决定长期记忆和遗忘掉一部分，这是这一部分保证了一些梯度的传递过程部分的缓解了梯度消失问题，比如如果 $z$ 都是 1 的话就不会存在有梯度消失的问题。短时记忆通常 $r$ 门是激活的。

   2. LSTM

      输入门，输出门，遗忘门
      $$
      i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t-1})\\
      f_t = \sigma(W^{(f)}x_t + U^{(f)}h_{t-1})\\
      o_t = \sigma(W^{(o)}x_t + U^{(o)}h_{t-1})\\
      \overline{c_t}=\tanh(W^{(c)}x_t + U^{(c)}h_{t-1})\\
      c_t = f_t \cdot c_{t-1} + i_t \cdot \overline{c_t}\\
      h_t = o_t \cdot \tanh(c_t)
      $$
      输入门对当前的信息的保留成都，遗忘门是对过去信息的保留程度，输出门是对一些内部不需要预测的信息的一种过滤机制(当前不需要这些信息用于对输出单词进行预测，但是不能遗忘只是对 softmax 输出预测没有用)。
