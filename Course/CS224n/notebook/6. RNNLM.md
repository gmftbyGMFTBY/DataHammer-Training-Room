## RNN 语言模型

1. Tradtional Language models

   语言模型的目的在于计算一个单词序列的产生概率，比如 $P(w_1,...,w_T)$ 语言模型在机器翻译和生成词选择方面非常有用。计算的概率往往都基于之前生成的 n 个 token 。

   为了简化问题，我们可以使用这样的马尔可夫假设

   $P(w_1,...,w_m)=\Pi_{i=1}^m P(w_i|w_1,...,w_{i-1})\approx \Pi_{i=1}^m P(w_i|w_{i-(n-1)},...,w_{i-1})$

   传统的机器学习方法一般这样的概率都是使用计数的方式计算的，比如

   $p(w_2|w_1)=\frac{count(w_1,w_2)}{count(w_1)},p(w_3|w_1,w_2)=\frac{count(w_1,w_2,w_3)}{count(w_1,w_2)}$

   随着 gram 的系数越大，需要存储的内容越多 

2. RNN

   1. 参数共享机制保证了模型的复杂度和语料的无关性
   2. 困惑度，困惑度越小越好 $2^{J}$ 其中 $J$ 是交叉熵的计算结果。
   3. 传统简单的 RNN 面临有严重的梯度消失和梯度下降问题，参见 Bengio et al 1994 的论文证明。
   4. 梯度爆炸: 梯度裁剪，梯度消失: ReLU  

3. RNNLM