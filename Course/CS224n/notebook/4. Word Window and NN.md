## Word Window 分类和神经网络

1. 分类

   * 交叉熵本是上在最小化分布的 KL 散度
   * 使用预训练的词向量可以提高模型的泛化能力，小数据集的深度模型很容易过拟合不要优化 w2v 但是如果数据集大的话在训练的过程中调整 w2v 效果可能会更好。

2. window classification 和交叉熵损失

   * 一般单词分类都是在上下文的环境下进行分类而不是单个分类

   * Window classification

     使用 softmax 方法对窗口的中心词进行分类，计算的时候将窗口内的所有单词词向量级联起来，然后使用最简单的 softmax 

3. 单层感知机

4. 最大间距损失和反向传播